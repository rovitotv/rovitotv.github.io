<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>rovitotv's blog - Research</title><link href="/" rel="alternate"></link><link href="/feeds/research.atom.xml" rel="self"></link><id>/</id><updated>2017-03-11T12:00:00-05:00</updated><entry><title>Efficient Generation of Image Chips for Training Deep Learning Algorithms</title><link href="/efficient-generation-of-image-chips-for-training-deep-learning-algorithms.html" rel="alternate"></link><published>2017-03-11T12:00:00-05:00</published><updated>2017-03-11T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2017-03-11:/efficient-generation-of-image-chips-for-training-deep-learning-algorithms.html</id><summary type="html">&lt;p&gt;Abstract: Training deep convolutional networks for satellite or aerial image
analysis often requires a large amount of training data. For a more robust
algorithm, training data need to have variations not only in the background and
target, but also radiometric variations in the image such as shadowing,
illumination changes, atmospheric …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Abstract: Training deep convolutional networks for satellite or aerial image
analysis often requires a large amount of training data. For a more robust
algorithm, training data need to have variations not only in the background and
target, but also radiometric variations in the image such as shadowing,
illumination changes, atmospheric conditions, and imaging platforms with
different collection geometry. Data augmentation is a commonly used approach to
generating additional training data. However, this approach is often
insufficient in accounting for real world changes in lighting, location or
viewpoint outside of the collection geometry. Alternatively, image simulation
can be an efficient way to augment training data that incorporates all these
variations, such as changing backgrounds, that may be encountered in real data.
The Digital Imaging and Remote Sensing Image Image Generation (DIRSIG) model is
a tool that produces synthetic imagery using a suite of physics-based radiation
propagation modules. DIRSIG can simulate images taken from different sensors
with variation in collection geometry, spectral response, solar elevation and
angle, atmospheric models, target, and background. For our research, we
selected ground vehicles as target objects and incorporated the Simulation of
Urban Mobility (SUMO) model into DIRSIG to generate scenes with vehicle
movement. SUMO is a multi-modal traffic simulation tool that explicitly models
vehicles that move through a given road network. Using the combination of
DIRSIG and SUMO, we can quickly generate hundreds of image chips, with the
target at the center with different backgrounds. The simulations generated
chips with vehicles and helicopters as targets, and corresponding images
without targets. Using parallel computing, 120,000 training images were
generated in about an hour. Some preliminary results show an improvement in the
deep learning algorithm when real image training data are augmented with the
simulated images.&lt;/p&gt;
&lt;p&gt;Links to paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1bWDcVBb0iuzpS0dbm4V-j5dbH_fE0lU-"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Electro-Optical Synthetic Civilian Vehicle Data Domes</title><link href="/electro-optical-synthetic-civilian-vehicle-data-domes.html" rel="alternate"></link><published>2012-07-01T12:00:00-04:00</published><updated>2015-12-25T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2012-07-01:/electro-optical-synthetic-civilian-vehicle-data-domes.html</id><summary type="html">&lt;p&gt;Abstract: This paper will look at using open source tools (Blender, LuxRender,
and Python) to generate a large data set to be used to train an object
recognition system. The model produces camera position, camera attitude, and
synthetic camera data that can be used for exploitation purposes. We focus on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Abstract: This paper will look at using open source tools (Blender, LuxRender,
and Python) to generate a large data set to be used to train an object
recognition system. The model produces camera position, camera attitude, and
synthetic camera data that can be used for exploitation purposes. We focus on
electro-optical (EO) visible sensors to simplify the rendering but this work
could be extended to use other rendering tools that support different
modalities. The key idea of this paper is to provide an architecture to produce
synthetic training data which is modular in design and constructed on open-
source off-the-shelf software yielding a physics accurate virtual model of the
object we want to recognize. For this paper the objects we are focused on are
civilian vehicles. This architecture shows how leveraging existing open-source
software allows for practical training of Electro-Optical object recognition
algorithms.&lt;/p&gt;
&lt;p&gt;Links to paper, presentation, and videos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1xd8LjM_iJUMLM0Cq8ZTYbsbsYjrp9Lq_"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1aZg3AL258UMy6o18o2iNkd_yvBvG8j-I"&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1BaDFeUhiOoxY6vKNd8CUvsUpdMJ7j74x"&gt;Video showing light conditions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1tYmoKI2f4L34fEPD14ARx0XHCL2Xyakf"&gt;Video showing positions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Image Processing in the Amazon Cloud with Enthought Python</title><link href="/image-processing-in-the-amazon-cloud-with-enthought-python.html" rel="alternate"></link><published>2012-07-01T12:00:00-04:00</published><updated>2015-12-25T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2012-07-01:/image-processing-in-the-amazon-cloud-with-enthought-python.html</id><summary type="html">&lt;p&gt;Abstract: Key idea of this work is to provide a open source frame work for
image processing that can be run on different parallel processing platforms.
Parallel processing is necessary because sensors are producing more data at
faster rates. This presentation looks at wide area motion imagery (WAMI) data
from …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Abstract: Key idea of this work is to provide a open source frame work for
image processing that can be run on different parallel processing platforms.
Parallel processing is necessary because sensors are producing more data at
faster rates. This presentation looks at wide area motion imagery (WAMI) data
from LAIR public released data set. Traditional S&amp;amp;E tools don’t scale in the
cloud in terms of performance or cost. Open source software along with cloud
computing becomes an enabler of affordable parallel processing because license
costs per node are minimized plus computation has become a commodity. This
presentation uses Enthought Python Distribution with parallel processing support
via built-in ZeroMQ. This process can be automated to produce large amounts of
data with minimal human effort.&lt;/p&gt;
&lt;p&gt;Links to paper and sample code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1ikj8W3poF0MxozazRzCriBdx1RRYHlm8"&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1Ss5Mi-9KnDd4NLyKOgPXJHr1tKkKVels"&gt;Frame Equalization (python code)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1UKjiGHkCDuoYBA9MfChwusUKw460rKpi"&gt;Frame Equalization Server (python code)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>SIFT Vehicle Recognition with Semi-Synthetic Model Database</title><link href="/sift-vehicle-recognition-with-semi-synthetic-model-database.html" rel="alternate"></link><published>2012-04-01T12:00:00-04:00</published><updated>2015-12-25T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2012-04-01:/sift-vehicle-recognition-with-semi-synthetic-model-database.html</id><summary type="html">&lt;p&gt;Object recognition with a semi-synthetic model database&lt;/p&gt;</summary><content type="html">&lt;p&gt;Abstract: Object recognition is an important problem that has many applications
that are of interest to the United States Air Force (USAF). Recently the USAF
released its update to Technology Horizons, a report that is designed to guide
the science and technology direction of the Air Force. Technology Horizons
specifically calls out for the need to use autonomous systems in essentially all
aspects of Air Force operations [1]. Object recognition is a key enabler to
autonomous exploitation of intelligence, surveillance, and reconnaissance (ISR)
data which might make the automatic searching of millions of hours of video
practical. In particular this paper focuses on vehicle recognition with Lowe’s
Scale-invariant feature transform (SIFT) using a model database that was
generated with semi-synthetic data. To create the model database we used a
desktop laser scanner to create a high resolution 3D facet model. Then the 3D
facet model was imported into LuxRender, a physics accurate ray tracing tool,
and several views were rendered to create a model database. SIFT was selected
because the algorithm is invariant to scale, noise, and illumination making it
possible to create a model database of only a hundred original viewing locations
which keeps the size of the model database reasonable.&lt;/p&gt;
&lt;p&gt;After a few years of other work this is my re-entry back into pattern
recognition.&lt;/p&gt;
&lt;p&gt;Links to paper and presentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1VZkgs6iEvn-FpeN_nRFcQdo_eZvt3Wfa"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1SfQY2eIqp8Hx-0-55kS-jQKIS6scx6vm"&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Physics Accurate Layered Sensing Model version 2 for ATR Center</title><link href="/physics-accurate-layered-sensing-model-version-2-for-atr-center.html" rel="alternate"></link><published>2011-08-01T12:00:00-04:00</published><updated>2015-12-25T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2011-08-01:/physics-accurate-layered-sensing-model-version-2-for-atr-center.html</id><summary type="html">&lt;p&gt;Learn how to build a layered sensing model with open source software version 2&lt;/p&gt;</summary><content type="html">&lt;p&gt;The main focus of this project was to provide an architecture for layered
sensing simulation. It is modular in design and constructed on open-source-off-
the-shelf-software (Blender, LuxRender, Python, OpenCV, and SUMO).&lt;/p&gt;
&lt;p&gt;The project was complied from previous work, fixing many errors such as,
multiple vehicle models and color projections. I teamed with and mentored two
undergraduate students throughout the process. Their effort enabled them to
increase their knowledge and experience with Linux, Python, Ray Tracing, and Geo-
Projections.&lt;/p&gt;
&lt;p&gt;Links to presentation and video:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1HG_FN2I4ETFxlyahPLL5DWWipSl7VqwB"&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1p64E5D1y_ijtSYFIUVs4AVb9nUTOyBEo"&gt;Building video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1254f2wC7PSu-TyHfq9xI9-hOwlPcU2H6"&gt;Medium flyer video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1qhYLMPl31a27Hzyhh4aePp9WU6zXZ8ut"&gt;Multi-car video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=10GukqBZk0nSkqJmtlE8J3oA-dcjD5ObK"&gt;Sadr City in Blender video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1CvdEVTdSSPX_g5mPI-o6rnfos-L7wFtn"&gt;Pursuer with five streams video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Physics Accurate Layered Sensing Model</title><link href="/physics-accurate-layered-sensing-model.html" rel="alternate"></link><published>2011-07-01T12:00:00-04:00</published><updated>2015-12-25T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2011-07-01:/physics-accurate-layered-sensing-model.html</id><summary type="html">&lt;p&gt;Learn how to build a layered sensing model with open source software&lt;/p&gt;</summary><content type="html">&lt;p&gt;Abstract: This paper will look at using open source tools (Blender [1],
LuxRender [2], Open CV [3], SUMO [4], and Python [5]) to build an image
processing model for exploring combinations of sensors/platforms for any given
image resolution. The model produces camera position, camera attitude, and
synthetic camera data that can be used for exploitation purposes. We focus on
electro-optical (EO) visible sensors to simplify the rendering but this work
could be extended to use other rendering tools that support different
modalities. Open Computer Vision (Open CV) is used to generate a camera model
for the intrinsics of the virtual camera. This camera model is then used to geo-
project the images from pixel space into a world coordinate system storing the
output in National Imagery Transmission Format (NITF) for display with standard
Intelligence Analysis (IA) tools such as RYA’s Pursuer [6]. We also demonstrate
Simulation of Urban Mobility (SUMO) software to simulate complex traffic
patterns. The key idea of the paper is to provide an architecture for layered
sensing simulation which is modular in design and constructed on open-source
off-the-shelf software yielding a physics accurate virtual model of the world.
This architecture shows how leveraging existing open-source software allows for
practical layered sensing modeling to be rapidly assimilated and utilized in
real-world applications. In this paper we demonstrate our model output is
automatically exploitable by using generated data with an innovative geo-
registration algorithm and real time display.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1QwYS7jOVuKRAbV4BbpaCxrw4BHV-fi56"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1-HbP96RIicmjOk2LXBh43HsmfFPbSk5Y"&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=13Z__K5z0GDeK3CycJ1_dGINmTfV2Usj5"&gt;Blender Camera Animation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1Ghg715obdFkE67dedvqkzrcejEYFFzeB"&gt;SUMO Animation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1MSfBBnPeviGvULip6PSwg8iPo6TzLegx"&gt;Sadr City Car Animation Wire Frame&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1WWTkWZya8_-yEPSjyPSSSDUa7YUq63Jr"&gt;Building Camera Animation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=15iIPhWcxeMPjmdeWH9gJFKkB7dh-lux6"&gt;Synthetic NITF in Pursuer Animation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Open Source Based Architecture For Layered Sensing Applications</title><link href="/open-source-based-architecture-for-layered-sensing-applications.html" rel="alternate"></link><published>2010-04-01T12:00:00-04:00</published><updated>2015-12-25T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2010-04-01:/open-source-based-architecture-for-layered-sensing-applications.html</id><summary type="html">&lt;p&gt;Using open source tools is an example of layered sensing applications&lt;/p&gt;</summary><content type="html">&lt;p&gt;Abstract: We present an architecture for layered sensing which is constructed
on open source and government off-the-shelf software. This architecture shows
how leveraging existing open-source software allows for practical graphical user
interfaces along with the underlying database and messaging architecture to be
rapidly assimilated and utilized in real-world applications. As an example of
how this works, we present a system composed of a database and a graphical user
interface which can display wide area motion imagery, ground-based sensor data
and overlays from narrow field of view sensors in one composite image composed
of sensor data and other metadata in separate layers on the display. We further
show how the development time is greatly reduced by utilizing open-source
software and integrating it into the final system design. The paper describes
the architecture, the pros and cons of the open-source approach with results for
a layered sensing application with data from multiple disparate sensors.
Keywords: SPADE, layered sensing, open-source, NASA World Wind, NITF, network
centric warfare&lt;/p&gt;
&lt;p&gt;Pursuer is based on NASA's World Wind and is a good example of Open Source
within the government. We are currently hosting the project on forge.mil with an
active open source DoD community. Links to paper, presentation, and code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=11f2TG64i2Bor_dHNq5dffNolPgvrdoBK"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1p_pdgQKrDM15xiDpGsFI1eGnZGER570K"&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1KV6Mk_DQbITUFW5hlTkV1r8LDnbvFg9H"&gt;Technology Milestone&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Antenna Aimpoint Integration For Staring Mode Surveillance</title><link href="/antenna-aimpoint-integration-for-staring-mode-surveillance.html" rel="alternate"></link><published>2008-07-01T12:00:00-04:00</published><updated>2015-12-25T12:00:00-05:00</updated><author><name>Todd V. Rovito</name></author><id>tag:None,2008-07-01:/antenna-aimpoint-integration-for-staring-mode-surveillance.html</id><summary type="html">&lt;p&gt;Air to ground data link that was capable of 250 Mbits/sec&lt;/p&gt;</summary><content type="html">&lt;p&gt;This was a fun project! We created a wireless air to ground data link that was
capable of 250 Mbits/sec. The same algorithm and math can be used for pointing
sensors.&lt;/p&gt;
&lt;p&gt;Abstract: Current persistent surveillance approaches require robust designs to
maintain a fixed operational picture. In this paper, we design, develop, and
demonstrate a feasible aimpoint solution. In the design, we derive the
mathematical transformation requirements to show a system-level design. Using
the transformations, we develop an operational methodology for real-time and
robust aimpoint solution that includes a ground antenna, and an aircraft with a
gimbal mounted camera and data link. Finally, we demonstrate a workable
prototype with real-world results. The AIMS methodology supports communication
timing constraints, a closed-loop feedback for error correction, and a succinct,
efficient, and effective method for maintaining persistent surveillance.&lt;/p&gt;
&lt;p&gt;Publicly released source code designed for 32 bit Linux RHEL 5. The code is
simple and should be easily adaptable to any operating system and hardware.&lt;/p&gt;
&lt;p&gt;Links to paper, presentation, and code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1fJwk4RL6R2GIBNObt7Hh85a6j_kHy9IT"&gt;Inexpensive Air To Ground Data Link Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1jcSl4LCTjcYYHJ9R3n7b3SUf71SapAkH"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/uc?id=1-1mJSYSfSbHq-yQ71KyTF5fKoE7b4Anj"&gt;Plane Point Public Released Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>